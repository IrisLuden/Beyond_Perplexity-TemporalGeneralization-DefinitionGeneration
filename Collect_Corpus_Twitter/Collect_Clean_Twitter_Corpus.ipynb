{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0271adb",
   "metadata": {},
   "source": [
    "# Twitter Corpus \n",
    "\n",
    "By: Iris Luden\n",
    "Last edited: March 2023\n",
    "\n",
    "Corpus 1: \n",
    "- Start date: 7-2015\n",
    "- End date: 4-2019 (included)\n",
    "\n",
    "Corpus 2: \n",
    "- Start date:    5-2019\n",
    "- End date: 2-2023 (included)\n",
    "\n",
    "# Data \n",
    "\n",
    "The tweets have been collected using Loureiro et al. (2022): \"TimeLMs: Diachronic Language models from Twitter\"\n",
    "\n",
    "Code can be found here: https://github.com/cardiffnlp/timelms\n",
    "\n",
    "Changes were made only  to \n",
    "\n",
    "They are stored in data/cleaned/tweets-{yyyy}-{months specification}.cleaned.jl \n",
    "\n",
    "\n",
    "\n",
    "### Tweet cleaning \n",
    "\n",
    "- lower case\n",
    "- tokenized using TweetTokenizer form nltk \n",
    "- stripped by strong.punctionation and ”“ characters\n",
    "\n",
    "#### Capping number of retrieved tweets\n",
    "Randomly remove some tweets from each year such that each month yields at most 52000 tweets.\n",
    "\n",
    "The reason is that when scraping using the API, some months retrieve more files than others, and we want them to remain roughly equal in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    word = word.strip('#\"$%&()*+,-/:;<=>@[\\]^_`{|}~”“')\n",
    "    return word \n",
    "    \n",
    "def cleanup_text(list_of_words):\n",
    "    ''' Clean up words in a splitted text, strip them from punctuation\n",
    "     remove empty word strings '''\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for word in list_of_words: \n",
    "        word = clean_word(word)\n",
    "        \n",
    "        if word != '':\n",
    "            cleaned_text.append(word)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def collect_clean_twitter(filename):\n",
    "    ''' Filters the tweets in filename \n",
    "    Removes tweets that are too small (less than 10 terms)\n",
    "    Replace odd characters with normal ones \n",
    "    Clean texts by removing punctionation \n",
    "    Tokenizes the sentences using TweetTokenizer of NLTK \n",
    "    Registers at which month the tweet was posted'''\n",
    "    \n",
    "    # read data\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    print(f\"There are {len(df)} tweets in total\")\n",
    "    \n",
    "    # remove tweets that are too small - saves computation time\n",
    "    df['Number of words'] = df['text'].map(lambda x: len(x.split()))\n",
    "    df = df[df['Number of words'] >= 10]\n",
    "    print(f'After removing tweets that are too small there are {len(df)} tweets left.')\n",
    "    \n",
    "    \n",
    "    # tokenize the tweets, and replace ’ character to the normal one \n",
    "    tokenizer = TweetTokenizer()\n",
    "    df['text tokenized'] = df['text'].map(lambda x: tokenizer.tokenize(x.replace(\"’\", \"'\")))\n",
    "    print(\"Completed tokenizing the tweets\")\n",
    "    \n",
    "    # clean the texts \n",
    "    df['text cleaned'] = df['text tokenized'].map(lambda x: cleanup_text(x))\n",
    "    df['Number of words'] = df['text cleaned'].map(lambda x: len(x))\n",
    "    \n",
    "    # remove the tweets with too little words \n",
    "    df = df[df['Number of words'] >= 10]\n",
    "    \n",
    "    print(f'After removing tweets that are too small there are {len(df)} tweets left.')\n",
    "    \n",
    "    # collect monthly data\n",
    "    df['month'] = df['created_at'].map(lambda x: x.month)\n",
    "    \n",
    "    df.drop(columns=['created_at', 'username'], inplace=True)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc045a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_tweets_per_month(df, cap=52000):\n",
    "    '''reduces the data frame to \"cap\" number of tweets per month. Keeps the corresponding ID.'''\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for month in df['month'].unique():\n",
    "        df_month = df[df['month'] == month]\n",
    "        print(len(df_month))\n",
    "        if len(df_month) > cap: \n",
    "            df_month_new = df_month.sample(n=cap)\n",
    "            all_dfs.append(df_month_new)\n",
    "            print(len(df_month_new))\n",
    "            print()\n",
    "            \n",
    "        else: \n",
    "            all_dfs.append(df_month)\n",
    "            \n",
    "    if len(all_dfs) != len(df['month'].unique()): \n",
    "        print(\"Not the correct number of dfs \")\n",
    "        return -1 \n",
    "    \n",
    "    # rejoin the dataframes back together\n",
    "    reduced_df = pd.concat(all_dfs)\n",
    "\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf2571",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9553acb",
   "metadata": {},
   "source": [
    "# Write the tweet sentences to the corpus files \n",
    "\n",
    "In the form: \n",
    "\n",
    "      sentence1 \\n\n",
    "      sentence2 \\n \n",
    "      sentence3 \\n\n",
    "      ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(df, outfile):\n",
    "    '''Write to files such that it can be read by Pathlinesentences for LSCD''' \n",
    "    with open('Twitter_Corpus/' + outfile, 'w', encoding='utf-8') as o: \n",
    "        print(outfile)\n",
    "        for text in df['text cleaned']:\n",
    "            \n",
    "            o.write(' '.join(text))\n",
    "            o.write('\\n')\n",
    "            \n",
    "    with open('Twitter_IDs/' + outfile[11:], 'w', encoding='utf-8') as o2: \n",
    "        print('Twitter_IDs/ids_' + outfile[11:])\n",
    "        for ID in df['id']: \n",
    "            o2.write(str(ID))\n",
    "            o2.write('\\n')\n",
    "            \n",
    "    print(\"Completed writing\", outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d56225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and reduce the tweet of each year and month \n",
    "\n",
    "for _, _, files in os.walk('data/cleaned'):\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        \n",
    "        # clean \n",
    "        df = collect_clean_twitter('data/cleaned/' + file)\n",
    "        \n",
    "        # data sets at most 52000 tweets, randomly remove some. \n",
    "        df_reduced = cap_tweets_per_month(df, cap=52000)\n",
    "\n",
    "        # determine outfile \n",
    "        year = file[7:-11]\n",
    "        if 'all' in year: \n",
    "            year = year[:-4]\n",
    "        outfile = f'Twitter_C1/tweets-{year}.txt'\n",
    "        \n",
    "#         # save reduced and cleaned file \n",
    "#         write_to_file(df_reduced, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38895d49",
   "metadata": {},
   "source": [
    "#### Count how many tweets are used per year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, _, files in os.walk('Twitter_IDs/'):\n",
    "    for filename in files[1:]: \n",
    "        year = filename[7:-4]\n",
    "        with open(path + filename, 'r') as o: \n",
    "            ids = o.readlines()\n",
    "            print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87955a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tweets per corpus \n",
    "C1 = [288000, 568609, 566447, 569045, 190000]\n",
    "C2 = [416000, 624000, 521669, 518255, 96084]\n",
    "print(sum(C1), sum(C2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd72ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pmaw\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import json \n",
    "import datetime as dt\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from helpers import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ccf7c",
   "metadata": {},
   "source": [
    "# Collecting Data from Pushshift API\n",
    "\n",
    "\n",
    "By: Iris Luden\n",
    "Date: March 2023 \n",
    "\n",
    "In this notebook, we to collect posts and comments from Reddit Pushshift API. We collect data from Januari 2015 until and including Februari 2023. This provides us two dta from a total of 92 months: 46 months overlapping with T5's pre-training period (until April 2019), and 46 months that are temporally shifted. \n",
    "\n",
    "#### Steps: \n",
    "\n",
    "1. Retrieve Reddit API key \n",
    "2. Collect data (posts and comments). N requests are made for each day of each month. \n",
    "3. Controll check: are there no duplicate IDs?\n",
    "\n",
    "#### Helper functions\n",
    "\n",
    "Helper functions used in this notebook can be found in helpers.py. This script includes the following functions: \n",
    "- correct_date(year, month, day): \n",
    "    - Returns True if the date exists, False otherwise\n",
    "- find_start_end_times(year, month, day):\n",
    "    - For a given day, returns the timestamp of this day and the next. If it is the last day of the month, returns None. \n",
    "- filter_text(text, word_threshold=10):\n",
    "    - Returns False if the text is too short (less than word_threshold), empty or removed\n",
    "- search_posts_month(api_praw, month, year, N=500):\n",
    "     - Requests at most N posts and N comments per day of the given month in the given year. Returns the texts and corresponding ids in lists\n",
    "- save_month(texts_list, text_ids, year, month): \n",
    "    - Given the results of a month, saves the texts and ids in the corresponding files\n",
    "    - Files are stored as follows: Reddit_data/{year}/id2text_{year}_{month}.json'\n",
    "- request_and_save(api_praw, year, N=500, start_month=1, end_month=12):\n",
    "    - Requests documents from Reddit for a given year. Saves the documents in a file per month.Requests for every dayof every month individually\n",
    "    \n",
    "    \n",
    "### Documentation\n",
    "\n",
    "Pmaw:  https://pypi.org/project/pmaw/\n",
    "\n",
    "Pushshift: https://github.com/pushshift/api#searching-submissions, https://github.com/pushshift/api\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a5b5a",
   "metadata": {},
   "source": [
    "#### 1. Retrieve Reddit API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up api to retrieve documents\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"insert client id here",\n",
    "    client_secret=\"insert key here",\n",
    "    password=\"insert password here",\n",
    "    user_agent= \"insert user agent here",\n",
    "    username=\"insert username here",\n",
    ")\n",
    "\n",
    "api_praw = PushshiftAPI(praw=reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0b21e",
   "metadata": {},
   "source": [
    "#### 2. Collecting Reddit Data \n",
    "1. Create directories \n",
    "\n",
    "2. For each day between April 2016 and Februari 2023, retrieve posts and comments usigng request_and_save. This will: \n",
    "    - Collect of each post/comment:\n",
    "        - date created \"created_utc\"\n",
    "        - id\n",
    "        - body/selftext\n",
    "    \n",
    "    - Exclude posts/comments that\n",
    "        - Contain less than 10 terms \n",
    "        - Are [removed] or [verwijderd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1246f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Create directories for each year. Uncomment to do so. \n",
    "\n",
    "# os.mkdir('Reddit_data/')\n",
    "\n",
    "# for year in range(2015, 2024):\n",
    "#     os.mkdir(f'Reddit_data/{year}')\n",
    "#     os.mkdir(f'Reddit_data/{year}/ids')    \n",
    "#     os.mkdir(f'Reddit_data/{year}/texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Collect posts/comments for each month of the yers 2016-2023\n",
    "\n",
    "for year in range(2016, 2023): \n",
    "    request_and_save(api_praw, year, N=500)\n",
    "    \n",
    "\n",
    "# Collect in 2015 for the months... \n",
    "request_and_save(api_praw, 2015, N=500, start_month=7, end_month=12)\n",
    "\n",
    "# Collect in 2023 for the months januari and februari\n",
    "request_and_save(api_praw, 2023, N=500, start_month=1, end_month=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc43e00",
   "metadata": {},
   "source": [
    "#### 3. Controll check for duplicate ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e6efa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining year  2015\n",
      "Duplicate ids in year  2015 False\n",
      "Examining year  2016\n",
      "Duplicate ids in year  2016 False\n",
      "Examining year  2017\n",
      "Duplicate ids in year  2017 False\n",
      "Examining year  2018\n",
      "Duplicate ids in year  2018 False\n",
      "Examining year  2019\n",
      "Duplicate ids in year  2019 False\n",
      "Examining year  2020\n",
      "Duplicate ids in year  2020 False\n",
      "Examining year  2021\n",
      "Duplicate ids in year  2021 False\n",
      "Examining year  2022\n",
      "Duplicate ids in year  2022 False\n",
      "Examining year  2023\n",
      "Duplicate ids in year  2023 False\n",
      "Duplicate ids? False\n"
     ]
    }
   ],
   "source": [
    "# check if there are not duplicate ids (I think not)\n",
    "all_ids = []\n",
    "\n",
    "for year in range(2015, 2024):\n",
    "\n",
    "    year_ids = []\n",
    "    \n",
    "    print(\"Examining year \", year)\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        try: \n",
    "            with open(f'Reddit_data/{str(year)}/ids/text_ids_{str(year)}_{str(month)}.txt', 'r') as f:\n",
    "\n",
    "                ids = f.readlines()\n",
    "                ids = [i.strip(\"\\n\") for i in ids]\n",
    "                year_ids += ids\n",
    "#                 print(\"Duplicate ids in month\", month, len(ids) != len(set(ids)))\n",
    "                # to check if the reading goes well\n",
    "        except: \n",
    "            pass\n",
    "                \n",
    "    print(\"Duplicate ids in year \", year, len(year_ids) != len(set(year_ids)))\n",
    "    all_ids += year_ids\n",
    "    \n",
    "print(\"Duplicate ids?\", len(all_ids) != len(set(all_ids)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

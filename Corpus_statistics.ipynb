{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus statistics \n",
    "\n",
    "By: Iris Luden\n",
    "Last edited: March 2023\n",
    "\n",
    "### Description \n",
    "\n",
    "Corpus1 and Corpus2 consist of Twitter and Reddit data.\n",
    "\n",
    "Corpus 1: \n",
    "- Start date: 07-2015\n",
    "- End date: 04-2019 (included)\n",
    "\n",
    "Corpus 2: \n",
    "- Start date:    05-2019\n",
    "- End date: 2-2023 (included)\n",
    "\n",
    "In this notebook, we collect:\n",
    "\n",
    "1. General statistics\n",
    "\n",
    "2. Neologisms / Emerging new words \n",
    "\n",
    "3. Trending candidate target words \n",
    "\n",
    "4. Stable candidate target words \n",
    "\n",
    "5. (extra) some example sentences for each emerging word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General statistics \n",
    "\n",
    "- 1.1 Total number of documents: 967400\n",
    "- 1.2 Number of sentences \n",
    "- 1.2 Sentence lengths \n",
    "- 1.3 Word frequencies & total number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import PathLineSentences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "minCount = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# determines whether the word should be included in the vocabulary \n",
    "def filter_rule(word):\n",
    "    ''' Returns False if a word is an url or an emoji '''\n",
    "    if 'https:/' in word or 'http://' in word or 'www.' in word: \n",
    "        return False\n",
    "    elif '\\\\u' in ascii(word): \n",
    "        return False\n",
    "    elif '\\\\U' in ascii(word): \n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def corpus_frequencies(corpuslines):\n",
    "    \n",
    "    freqs_dict = Counter()\n",
    "    \n",
    "    for sentence in corpuslines:\n",
    "        for word in sentence: \n",
    "            if filter_rule(word):\n",
    "                freqs_dict[word] += 1\n",
    "    return freqs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data (splits sentences automatically, and splits into words by space)\n",
    "texts_C1 = PathLineSentences('Corpora/Corpus1')\n",
    "texts_C2 = PathLineSentences('Corpora/Corpus2')\n",
    "\n",
    "# sentence lengths\n",
    "sentence_lengths_C1 = [len(t) for t in texts_C1]\n",
    "sentence_lengths_C2 = [len(t) for t in texts_C2]\n",
    "\n",
    "# number of sentences\n",
    "number_of_sentences_C1 = len(sentence_lengths_C1)\n",
    "number_of_sentences_C2 = len(sentence_lengths_C2)\n",
    "\n",
    "# Total number of terms  \n",
    "number_of_terms_C1 = sum(sentence_lengths_C1)\n",
    "number_of_terms_C2 = sum(sentence_lengths_C2)\n",
    "\n",
    "# Word frequency of each corpus \n",
    "C1_freqs = corpus_frequencies(texts_C1)\n",
    "C2_freqs = corpus_frequencies(texts_C2)\n",
    "total_freqs = C1_freqs + C2_freqs\n",
    "\n",
    "# Total number of terms in the corpora \n",
    "total_words_C1 = sum(C1_freqs.values())\n",
    "total_words_C2 = sum(C2_freqs.values())\n",
    "print(len(total_freqs))\n",
    "\n",
    "# Reduce by mminimum count = 30\n",
    "# Onnly include words that occur at least 30 times in the corpus\n",
    "print(\"MinCount is \", minCount)\n",
    "C1_freqs_reduced  = Counter({key:value for key, value in C1_freqs.items() if value > minCount})\n",
    "C2_freqs_reduced  = Counter({key:value for key, value in C2_freqs.items() if value > minCount})\n",
    "total_freqs_reduced = C1_freqs_reduced + C2_freqs_reduced\n",
    "print(len(total_freqs_reduced))\n",
    "\n",
    "# Words that occur in both corpora\n",
    "intersection = C1_freqs_reduced.keys() & C2_freqs_reduced.keys()\n",
    "print(len(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus data in dataframe \n",
    "corpus_df = pd.DataFrame({'Corpus 1': {'Number of sentences': number_of_sentences_C1,\n",
    "                          'Average sentence length': np.mean(sentence_lengths_C1),\n",
    "                          'Number of terms': number_of_terms_C1, \n",
    "                          'Number of words': total_words_C1,            \n",
    "                          'Unique words': len(C1_freqs), \n",
    "                          'Unique words (reduced)': len(C1_freqs_reduced)},\n",
    "             'Corpus 2': {'Number of sentences': number_of_sentences_C2,\n",
    "                          'Average sentence length': np.mean(sentence_lengths_C2),\n",
    "                          'Number of terms': number_of_terms_C2, \n",
    "                          'Number of words': total_words_C2,   \n",
    "                          'Unique words': len(C2_freqs),\n",
    "                          'Unique words (reduced)': len(C2_freqs_reduced)\n",
    "                         }, \n",
    "              'Combined': {'Number of sentences': number_of_sentences_C2 + number_of_sentences_C1,\n",
    "              'Average sentence length': np.mean(sentence_lengths_C2 + sentence_lengths_C1),\n",
    "              'Number of terms': number_of_terms_C2 + number_of_terms_C1, \n",
    "              'Number of words': sum(total_freqs.values()),   \n",
    "              'Unique words': len(total_freqs),\n",
    "              'Unique words (reduced)': len(total_freqs_reduced)\n",
    "             }})\n",
    "\n",
    "# add sums \n",
    "display(corpus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify in which sentences wach target word occurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the line number and the target words \n",
    "def collect_corpus_index(sentences):\n",
    "    '''Maps every word to the set of line numbers on which they occur'''\n",
    "    words2lines = {}\n",
    "    i = 0\n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if filter_rule(word):\n",
    "                if word not in words2lines: \n",
    "                    words2lines[word] = set()\n",
    "                words2lines[word].add(i)\n",
    "        i += 1 \n",
    "    return words2lines\n",
    "\n",
    "w2l_C1 = collect_corpus_index(texts_C1)\n",
    "w2l_C2 = collect_corpus_index(texts_C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence frequency for each word \n",
    "sentence_freqs_C1 = Counter({word: len(w2l_C1[word]) for word in w2l_C1})\n",
    "sentence_freqs_C2 = Counter({word: len(w2l_C2[word]) for word in w2l_C2})\n",
    "\n",
    "sentence_freqs_C1_reduced = Counter({word: len(w2l_C1[word]) for word in C1_freqs_reduced})\n",
    "sentence_freqs_C2_reduced = Counter({word: len(w2l_C2[word]) for word in C2_freqs_reduced})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Emerging words selection\n",
    "\n",
    "\n",
    "Lazaridou et. al. (2021) define **emerging new words** \"as those that occur frequently on the test set (at least 50 times), but either: (i) were previously unseen on the training set, or (ii) occurred much less frequently on the training set than on the test set, as indicated by an at least 5 times lower unigram probability.\"\n",
    "\n",
    "Hence emerging (new) words are words that are either unseen in  $C_1$ , and seen in $C_2$, or words whos frequency has significantly increased. \n",
    "\n",
    "We select words as neologisms if their frequency in $C_1$ is below 15, OR if their frequency in $C_2$ is at least five times higher than in $C_1$. Additionally, an emerging new word should occur at least 50 times in $C_2$ - such that it provides us with sufficient example context sentences. \n",
    "\n",
    "In this script we also look at weak/strong neologisms. \n",
    "- Weak neologisms: occur 5 times more in $C_2$ than in $C_1$, but may occur (a few times) in $C_1$ as well\n",
    "- Strong neologisms:  occur 5 times more in $C_2$ than in $C_1$, and do NOT occur in $C_1$\n",
    "\n",
    "\n",
    "#### Additional info\n",
    "\n",
    "We disregard words that contain digits, URLs, or emoji's. Additionally, we check whether they are in the WordNet database. This info is saved to the files:\n",
    "\n",
    "    - 'Targetwords/neologisms.tsv'\n",
    "    - 'Targetwords/weak_neologisms.tsv'\n",
    "    - 'Targetwords/all_neologisms_{minCount}.tsv'\n",
    "\n",
    "The files are saved to ../LSCDetection-master/testsets/test/all_neologisms_reduced.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "def has_numbers(input_string):\n",
    "    ''' return True if the word does not contain any digits '''\n",
    "    return any(char.isdigit() for char in input_string)\n",
    "\n",
    "\n",
    "def in_wordnet(word):\n",
    "    ''' return true if the word is known in WordNet database'''\n",
    "    if wordnet.synsets(word) != []:\n",
    "        return True\n",
    "    return False \n",
    "    \n",
    "\n",
    "# Other neologisms: words whose use have increased drastically. \n",
    "def find_neologisms(freqs1, freqs2, thres=50, ratio=5):\n",
    "    ''' Find new emerging words from corpus frequencies\n",
    "    neologisms: do not occur in C1, and occur in C2 at least minCount times\n",
    "    weak neologisms: occur at least 5 times more in C2 compared to C1'''\n",
    "    strict_neologisms = []\n",
    "    weak_neologisms = []\n",
    "\n",
    "    # go over the terms in the recent vocabulary\n",
    "    for word in freqs2:\n",
    "        \n",
    "        # skip all the non-relevant words\n",
    "        if filter_rule(word) and not has_numbers(word):\n",
    "            \n",
    "            if freqs2[word] >= thres:\n",
    "\n",
    "                # strict neologism\n",
    "                if word not in freqs1:\n",
    "                    strict_neologisms.append((word, freqs1[word], freqs2[word]))\n",
    "\n",
    "                # weak neologisms\n",
    "                else: \n",
    "                    if freqs1[word]  * ratio <= freqs2[word]:\n",
    "                        weak_neologisms.append((word, freqs1[word], freqs2[word]))\n",
    "\n",
    "    return strict_neologisms, weak_neologisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find neologisms/emerging (new) words \n",
    "strict_neologisms, weak_neologisms = find_neologisms(sentence_freqs_C1, sentence_freqs_C2)\n",
    "print(len(strict_neologisms), len(weak_neologisms))\n",
    "\n",
    "# find neologisms based on SENTENCE/(document) FREQUENCIES in stead of WORD FREQUENCIES\n",
    "df_neologisms = pd.DataFrame(strict_neologisms, columns = ['Term', 'C1 freq', 'C2 freq'])\n",
    "df_weak_neologisms = pd.DataFrame(weak_neologisms, columns = ['Term', 'C1 freq', 'C2 freq'])\n",
    "\n",
    "# see whether they are in WordNet or not\n",
    "df_neologisms['Wordnet'] = df_neologisms['Term'].apply(lambda x: in_wordnet(x))\n",
    "df_weak_neologisms['Wordnet'] = df_weak_neologisms['Term'].apply(lambda x: in_wordnet(x))\n",
    "print(\"Number of words that occur in WordNet \", sum(df_neologisms['Wordnet']), sum(df_weak_neologisms['Wordnet']))\n",
    "\n",
    "display(df_neologisms.head(10))\n",
    "display(df_weak_neologisms.head(10))\n",
    "\n",
    "print(len(df_neologisms), len(df_weak_neologisms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframes to csv files. \n",
    "### NOTE: uncomment to save\n",
    "\n",
    "# os.mkdir('Targetwords/')\n",
    "\n",
    "\n",
    "# df_neologisms.to_csv(f'Targetwords/neologisms_{minCount}.tsv', sep='\\t', index=False)\n",
    "# df_weak_neologisms.to_csv(f'Targetwords/weak_neologisms_{minCount}.tsv', sep='\\t', index=False)\n",
    "\n",
    "# merge together & Save\n",
    "# df_all_neologisms = pd.concat([df_neologisms, df_weak_neologisms], ignore_index=True)\n",
    "# df_all_neologisms.to_csv(f'Targetwords/all_neologisms_{minCount}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Calculate target words based on trending scores\n",
    "\n",
    "**Trending scores** based on the total frequency in each corpus (Chen et al. 2022)\n",
    "\n",
    "- Can only be defined for words in both corpora\n",
    "- Can only be accurate when the corpora have roughly the same number of words/documents\n",
    "\n",
    "$$ score(n) =  \\frac{F_{C2}(w) - F_{C1}(w)}{F_{C1}(w) + k}$$ \n",
    "\n",
    "The variable $k$ is meant to filter out words with high overall frequency. \n",
    "\n",
    "\n",
    "#### Exclusions \n",
    "\n",
    "We only consider words that:\n",
    "- occur in both $C_1$ and $C_2$ at least 30 times \n",
    "- have a wordnet entry \n",
    "- are not digits, emoji's, URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_score(word, freqs1, freqs2, k=15):\n",
    "    '''Calculate the trending score of a wrod\n",
    "        based on its frequency at two time periods'''\n",
    "    return (freqs2[word] - freqs1[word]) / (freqs1[word] + k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trending score for each word in the common vocabulary of C1 and C2\n",
    "intersection = C1_freqs_reduced.keys() & C2_freqs_reduced.keys()\n",
    "\n",
    "words_trending_scores = []\n",
    "for word in intersection:\n",
    "    \n",
    "    if in_wordnet(word) and has_numbers(word) == False:\n",
    "        \n",
    "        # only words with sufficient frequency in both corpora\n",
    "        if sentence_freqs_C1[word] >= minCount and sentence_freqs_C2[word] >= minCount: \n",
    "            words_trending_scores.append((word,\n",
    "                                          trending_score(word, sentence_freqs_C1, sentence_freqs_C2, k=minCount), \n",
    "                                         sentence_freqs_C1[word], sentence_freqs_C2[word]))\n",
    "\n",
    "df_trending_scores = pd.DataFrame(words_trending_scores, \n",
    "                                  columns=['word', 'trending score', 'Sentence freq C1', 'Sentence freq C2'])\n",
    "df_trending_scores.sort_values(by='trending score', ascending=False, inplace=True)\n",
    "display(df_trending_scores)\n",
    "\n",
    "# 1. fselect candidate trending words based on threshold\n",
    "threshold = 1\n",
    "df_candidate_trending = df_trending_scores[df_trending_scores['trending score'] >= threshold]\n",
    "display(df_candidate_trending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Candidate changing target words \n",
    "\n",
    "# ### NOTE: uncomment to run ###\n",
    "\n",
    "# outfile_candidate_trending = f'Targetwords/trending_candidates_statistics_{minCount}_{threshold}.tsv'\n",
    "# df_candidate_trending.to_csv(outfile_candidate_trending, sep='\\t', index=False)\n",
    "\n",
    "# # save in file format for LSCD \n",
    "# df_save = pd.DataFrame([df_candidate_trending['word'], df_candidate_trending['word']]).T\n",
    "# display(df_save.head(10))\n",
    "\n",
    "# df_save.to_csv(f'Targetwords/LSCD_trending_candidates_{minCount}_{threshold}.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stable Candidate word selection \n",
    "\n",
    "Randomly select 1000 words from the corpus, given that they: \n",
    "- are in the wordnet database\n",
    "- do not contain any digits, emoji's, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the intersecting words that occur in both corpora\n",
    "intersection = C1_freqs_reduced.keys() & C2_freqs_reduced.keys()\n",
    "print(len(intersection))\n",
    "\n",
    "# words that have been varified in wordnet and don't contain digits\n",
    "verified_intersection = [word for word in intersection if (not has_numbers(word) and in_wordnet(word)\\\n",
    "                                                           and sentence_freqs_C1[word] >= minCount \\\n",
    "                                                           and sentence_freqs_C2[word] >= minCount)]\n",
    "\n",
    "candidate_stable = random.sample(verified_intersection, 1000)\n",
    "print(len(candidate_stable)) # should be 1000 \n",
    "\n",
    "# save in format for LSCD \n",
    "df_save = pd.DataFrame([candidate_stable, candidate_stable]).T\n",
    "display(df_save.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE (commented out due to randomness)\n",
    "# df_save.to_csv(f'Targetwords/stable_candidates_{minCount}.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary: \n",
    "\n",
    "In this notebook, we collected:\n",
    "- Corpus statstics\n",
    "- Neologisms between C1 and C2 \n",
    "- Candidate target words based on trending scores \n",
    "- Candidate stable words based on random selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: \n",
    "Collect all cadidate target words, without the \"trending rule\". \n",
    "Saved in 'Targetwords/all_candidates_{minCount}.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the intersecting words that occur in both corpora\n",
    "intersection = C1_freqs_reduced.keys() & C2_freqs_reduced.keys()\n",
    "print(len(intersection))\n",
    "\n",
    "# words that have been varified in wordnet and don't contain digits\n",
    "verified_intersection = [word for word in intersection if (not has_numbers(word) and in_wordnet(word)\\\n",
    "                                                           and sentence_freqs_C1[word] >= minCount \\\n",
    "                                                           and sentence_freqs_C2[word] >= minCount)]\n",
    "df_all_words = pd.DataFrame([verified_intersection, verified_intersection]).T\n",
    "\n",
    "# df_all_words.to_csv(f'Targetwords/all_candidates_{minCount}.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Extra: Collect example sentences for each neologism\n",
    "\n",
    "The script below collects some example sentences for the neologisms such that we can manually select 20 emerging new target words. \n",
    "\n",
    "For this we use the w2l dictionaries created at part 1 of this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read corpus original lines \n",
    "import os\n",
    "\n",
    "def read_lines(foldername):\n",
    "    \n",
    "    for _, _, files in os.walk(foldername):\n",
    "        data_lines = []\n",
    "        for file in files:\n",
    "            with open(foldername+ file, 'r', encoding='utf-8') as f: \n",
    "                data_lines += f.readlines()                \n",
    "    return data_lines\n",
    "\n",
    "lines_C2 = read_lines('Corpora/Corpus2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_examples(word, w2l, corpuslines, minCount=30, n=5):\n",
    "    '''Retreive sentences containing the target word. \n",
    "    The sentences are detokenized. \n",
    "    Only returns example sentences that contain less than 200 words '''\n",
    "    \n",
    "    example_lines = w2l[word]\n",
    "    \n",
    "    example_sentences = [corpuslines[l] for l in example_lines]\n",
    "    example_sentences = set(example_sentences) # set to filter out duplicates\n",
    "\n",
    "    # in case the term has too little examples in the corpus\n",
    "    if len(example_sentences) < minCount:\n",
    "        return None\n",
    "    \n",
    "    return list(example_sentences)[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the annotation, write 5 example sentences \n",
    "df_all_neologisms = pd.read_csv(f'Targetwords/all_neologisms_{minCount}.tsv', sep='\\t')\n",
    "df_all_neologisms['Examples'] = df_all_neologisms['Term'].map(lambda x: retrieve_examples(x, w2l_C2, lines_C2))\n",
    "df_all_neologisms.dropna(inplace=True)\n",
    "display(df_all_neologisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file for annotators\n",
    "\n",
    "# ### Note: uncomment to save ### \n",
    "\n",
    "# df_all_neologisms.to_csv(f'Targetwords/all_neologisms-ANNOTATORS.tsv', sep='\\t', encoding='utf-8', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13db1fcd",
   "metadata": {},
   "source": [
    "# Pre-processing and cleaning the Reddit data\n",
    "\n",
    "By: Iris Luden \n",
    "\n",
    "Last edited: 27-03-2023\n",
    "\n",
    "\n",
    "\n",
    "The retrieved Reddit posts and comments are stored in the folder:\n",
    "\n",
    "'Reddit_data'\n",
    "\n",
    "    |__> 2016\n",
    "        |__> ids\n",
    "            |__> ids_2016_1.txt\n",
    "            |__> ids_2016_2.txt\n",
    "            |__> ...\n",
    "        |__> texts\n",
    "            |__> texts_2016_1.txt\n",
    "            |__> texts_2016_2.txt\n",
    "            |__> ...\n",
    "        |__> texts_clean*\n",
    "            |__> Monthly_freqs_2016_1.json* \n",
    "            |__> ...\n",
    "            |__> {year}_{month}_cleaned_texts.json* \n",
    "            |__> ... \n",
    "    |__> 2017\n",
    "    |__> 2018\n",
    "    |__> ... \n",
    "    |__> 2023\n",
    "\n",
    "The clean texts are being written into the folder year/texts_clean/...\n",
    "The monthly frequency counts of the words are also stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6fcb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helpers import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70552dd",
   "metadata": {},
   "source": [
    "# 1. Cleaning \n",
    "\n",
    "1. Filtering Non-English: \n",
    "    \n",
    "    The text are filtered for non-English words using `filer_non_English` in helpers.py. This function checks if any of the stopwords is in the text. If not, the text is not added to the clean_texts file. Inspired by [TO DO]\n",
    "    \n",
    "2. Sentence Tokenizing & Word tokenizing \n",
    "    - The texts are tokenized into sentences using the standard `nltk.tokenize.sent_tokenize`\n",
    "    - Sentences are tokenized into words using the standard `nltk.tokenize.TreebankWordTokenizer()`` \n",
    "        - [TO DO] Another option could have been to use the TweetTokenizer  https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9\n",
    "\n",
    "3. Cleaning punctuation\n",
    "    - Special punctuations ’ and … are replaced by default punctuation ' and ... \n",
    "    - All words are stripped from punctuation from `string.punctuation` and the characters ”“\n",
    "    \n",
    "4. Lower case \n",
    "\n",
    "5. Disregard posts/comments of less than 10 terms, separated by spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f387d91",
   "metadata": {},
   "source": [
    "#### A: Clean data of 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "start = time.time()\n",
    "\n",
    "for month in range(7, 13):\n",
    "    collect_clean_texts(year, month)\n",
    "end = time.time()\n",
    "print(f'Cleaning the year {year} took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142245b",
   "metadata": {},
   "source": [
    "#### B: Clean data of full years 2016-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf376a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# years 2016-2022\n",
    "for year in range(2016, 2023): \n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # loop over the months of the year, clean the texts, and place in the \n",
    "    for month in range(1,13):\n",
    "        collect_clean_texts(year, month)\n",
    "        \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Cleaning the year {year} took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfd3bb",
   "metadata": {},
   "source": [
    "#### C:  Clean data for the year 2023, months 1, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ec981",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "start = time.time()\n",
    "# loop over the months of the year\n",
    "for month in range(1, 3):\n",
    "    collect_clean_texts(year, month)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(f'Cleaning the year {year} took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9578b",
   "metadata": {},
   "source": [
    "# Move the files into the two corpus directories \n",
    "\n",
    "Corpus 1:  July 2015 until and including April 2019 (total 46 months)\n",
    "\n",
    "Coprus 2: May 2019 until Februari 2023 (total 46 months)\n",
    "\n",
    "#### Divide and sort cleaned data in two folders\n",
    "    - Reddit_data/Corpus1\n",
    "    - Reddit_data/Corpus2\n",
    "\n",
    "These will be moved to the general folders Corpus1 and Corpus 2 together with the twitter data set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceded211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('Reddit_data/Corpus1')\n",
    "# os.mkdir('Reddit_data/Corpus2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CORPUS 1\n",
    "# year = 2015\n",
    "# for month in range(7, 13):\n",
    "#     old_path = f'Reddit_data/{year}/texts_clean/{year}_{month}_cleaned_texts.txt'\n",
    "#     new_path = f'Reddit_data/Corpus1/{year}_{month}_cleaned_texts.txt'\n",
    "#     os.replace(old_path, new_path)\n",
    "        \n",
    "# ## corpus 1 for t5: all documents from 2016-01 until  2019-04\n",
    "# for year in range(2016, 2020): \n",
    "    \n",
    "#     for month in range(1, 13):\n",
    "        \n",
    "#         if year == 2019 and month > 4: \n",
    "#             break \n",
    "#         else: \n",
    "#             old_path = f'Reddit_data/{year}/texts_clean/{year}_{month}_cleaned_texts.txt'\n",
    "#             new_path = f'Reddit_data/Corpus1/{year}_{month}_cleaned_texts.txt'\n",
    "#             os.replace(old_path, new_path )\n",
    "\n",
    "# # CORPUS 2\n",
    "# for year in range(2019, 2024): \n",
    "    \n",
    "#     for month in range(1, 13):\n",
    "#         if year == 2019 and month < 5: \n",
    "#             pass\n",
    "#         elif year == 2023 and month > 2: \n",
    "#             break \n",
    "#         else: \n",
    "#             old_path = f'Reddit_data/{year}/texts_clean/{year}_{month}_cleaned_texts.txt'\n",
    "#             new_path = f'Reddit_data/Corpus2/{year}_{month}_cleaned_texts.txt'\n",
    "#             os.replace(old_path, new_path )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

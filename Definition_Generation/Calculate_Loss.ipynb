{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Loss and Perplexity for the example sentences \n",
    "\n",
    "By: Iris Luden \n",
    "Date created: May 2023 \n",
    "\n",
    "This notebook investigates methods to obtain the cross-entropy loss, perplexity scores, and pseudo-perplexity scores for my data under investigation. \n",
    "\n",
    "The goal is to calculate the (1) cross entropy loss (2) perplexity scores, (3) pseudo-log likelihoods:\n",
    "- For each target, context pair \n",
    "- For each definition\n",
    "- For all 400 the annotated example sentences\n",
    "- For all randomly sampled example sentences \n",
    "\n",
    "\n",
    "# Background \n",
    "\n",
    "#### A: Loss context/example sentences \n",
    "\n",
    "Calculate loss of the model on the context sentence of a target word\n",
    "\n",
    "#### B: Perplexity over the entire sentences: \n",
    "\n",
    "    Given model $M$, and input sequence $Q$, we calculate the perplexity as follows: \n",
    "    $$PPL(Q) = exp^{H(M, Q)}$$ \n",
    "    where $H(M, Q)$ is the models cross-entropy for the input sequence Q.\n",
    "\n",
    "#### C: Pseudo-perplexity or pseudo-log-likelihood, defined as: \n",
    "\n",
    "    $$ PLL(Q) := \\sum^{|Q|}_{t=1} \\log P_{M} (w_t|Q \\setminus w_{t})$$ \n",
    "    \n",
    "This is probability P_{M} is calculated differently depending on the model:\n",
    "In the case of autoregressive models, it only takes into account the left-side context. \n",
    "\n",
    "The psuedo-perplexity for 1 sequence is equal to the PLL. One can also calculate the pseudo-perplexity over an entire corpus, by:\n",
    "\n",
    "$$ PPPL(C) := exp (-\\frac{1}{N} \\sum_{Q \\in C} PPL(Q) $$ \n",
    "\n",
    "Where $N$ denotes the number of sentences in C. \n",
    "\n",
    "#### D: masked target word prediction Loss \n",
    "\n",
    "The perplexity of target word $w$ in context sequence $Q$:\n",
    "$$ PPL(Q, w_t) \\approx exp^{H(M, Q, w_t)}$$\n",
    "\n",
    "Where I define H(M, Q, w_t) as the loss of the task of predicting target word $w$ in the sequence $Q$ where $w$ is masked. \n",
    "\n",
    "# Description \n",
    "\n",
    "#### Part 1: Scores for the example sentences\n",
    "\n",
    "Of each targetword (60 in total), and for each corpus, at most 50 sentences were sampled. For these total +=8000 example sentences, we:\n",
    "    - scores compute\n",
    "    - analysis: correlation with corpus & visualizations\n",
    "    \n",
    "    \n",
    "#### Part 2: Scores for annotated example sentences \n",
    "\n",
    "Of each targetword (60 in total), and for each corpus, 5 sentences were judged by annotators. \n",
    "We compare the loss/perplexity/psuedoperplexity scores with the correctness of the judgements. \n",
    "    - scores compute\n",
    "    - analysis: correlations & visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model \n",
    "\n",
    "model_id = \"t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "# prepare the data\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read complete data set \n",
    "df = pd.read_csv('All_DG_results.tsv', sep='\\t')\n",
    "display(df.head())\n",
    "\n",
    "# retrieve categories \n",
    "stable = list(pd.read_csv('Targetwords/Stable_targets_20.tsv', sep='\\t')['Word'])\n",
    "changing = list(pd.read_csv('Targetwords/Changing_targets_20.tsv')['Word'])\n",
    "emerging = list(pd.read_csv('Targetwords/Emerging_targets_20.tsv', sep='\\t')['Word'])\n",
    "\n",
    "def categorize(x, changing, emerging, stable):\n",
    "    if x in changing:\n",
    "        return 'changing'\n",
    "    elif x in emerging:\n",
    "        return 'emerging'\n",
    "    elif x in stable:\n",
    "        return 'stable'\n",
    "    else: \n",
    "        return None \n",
    "    \n",
    "# only inlcude the 20 target words used in the experiments \n",
    "df['Experiment set'] = df['Word'].map(lambda x: categorize(x, changing, emerging, stable_all))\n",
    "display(df)\n",
    "print(len(df))\n",
    "df.dropna(inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "Cross-entropy loss and Perplexity for all 8000+ sampled sentences \n",
    "\n",
    "Sources: \n",
    "https://huggingface.co/docs/transformers/model_doc/t5\n",
    "https://huggingface.co/docs/transformers/perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(encoding, model):\n",
    "    ''' Calculates the negalive-log-likelihood and perplexity \n",
    "    for the encoded sequence \"encoding\" and the model'''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoding.input_ids, labels=encoding.input_ids)\n",
    "        \n",
    "    return (outputs.loss).item()\n",
    "\n",
    "def nll_normalized(encoding, model):\n",
    "    ''' Calculates the negalive-log-likelihood and perplexity \n",
    "    for the encoded sequence \"encoding\" and the model'''\n",
    "    \n",
    "    seq_length = encoding.input_ids.size(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoding.input_ids, labels=encoding.input_ids)\n",
    "        \n",
    "    return ((outputs.loss)/seq_length).item()\n",
    "\n",
    "def loss_in_context(sentence, target_word, tokenizer, model):\n",
    "    ''' Calculates the perpllexity of the model of the target word in the sentence\n",
    "    By calculating the loss when predicting the masked target word'''\n",
    "    \n",
    "    masked_encoding = tokenizer(sentence.replace(target_word, '<extra_id_0>'), return_tensors=\"pt\")\n",
    "    \n",
    "    labels = ' '.join([f'<extra_id_{i}>' \n",
    "                       if (target_word not in sentence.split()[i])  else target_word \n",
    "                       for i in range(len(sentence.split()))])\n",
    "    labels = tokenizer(labels, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_encoding.input_ids, labels=labels.input_ids)\n",
    "        \n",
    "    return outputs.loss.item()\n",
    "    \n",
    "def pseudo_perplexity2(sentence, model, tokenizer):\n",
    "    ''' calculates the pseudo log likelihood for a sequence. \n",
    "    sums over the log probability of each token in the sequence, \n",
    "    conditioned on the context on both sides.\n",
    "    NOTE: THE MASKING IS NOT GOING CORRECTLY. SHOULD BE REVISED'''\n",
    "    all_nll = []\n",
    "    \n",
    "    splitted_sentence = sentence.split()\n",
    "    \n",
    "    for index in range(len(splitted_sentence)):\n",
    "        \n",
    "        target_word = splitted_sentence[index]\n",
    "        \n",
    "        masked_encoding = tokenizer(sentence.replace(target_word, '<extra_id_0>'), return_tensors=\"pt\")\n",
    "        \n",
    "        labels = ' '.join([f'<extra_id_{i}>' \n",
    "                           if (target_word != sentence.split()[i])  else target_word \n",
    "                           for i in range(len(sentence.split()))])\n",
    "\n",
    "        labels = tokenizer(labels, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(masked_encoding.input_ids, labels=labels.input_ids)\n",
    "        \n",
    "        all_nll.append(outputs.loss)\n",
    "    \n",
    "    return sum(all_nll).item()\n",
    "\n",
    "def collect_perplexities(df, model, tokenizer):\n",
    "    \n",
    "    words_nll = []\n",
    "    examples_nll = []\n",
    "    \n",
    "    for target_word, example in tqdm(df[['Word', 'Example']].values):\n",
    "        \n",
    "        words_nll.append(loss_in_context(example,  target_word, tokenizer, model))\n",
    "        \n",
    "        example_encoding = tokenizer(example, return_tensors=\"pt\")\n",
    "        examples_nll.append(nll(example_encoding, model))\n",
    "        \n",
    "    return words_nll, examples_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexities \n",
    "words_nll, examples_nll = collect_perplexities(df, model, tokenizer)\n",
    "\n",
    "# Create a df of results \n",
    "\n",
    "results_df = df.copy()\n",
    "results_df['Words NLL'] = words_nll\n",
    "results_df['Words PPL'] = results_df['Words NLL'].map(lambda x: np.exp(x))\n",
    "results_df['Examples NLL'] = examples_nll\n",
    "results_df['Examples PPL'] = results_df['Examples NLL'].map(lambda x: np.exp(x))\n",
    "\n",
    "display(results_df.head())\n",
    "\n",
    "# save to csv file\n",
    "# results_df.to_csv('Perplexity_DG_results_first_20_target_words.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Of the 60 target words with 100 example sentences each. \n",
    "\n",
    "1. Compute correlations \n",
    "2. Create visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read perplexities \n",
    "filename = 'Perplexity_DG_results_first_20_target_words.tsv'\n",
    "\n",
    "ppl_df = pd.read_csv(filename, sep='\\t')\n",
    "display(ppl_df)\n",
    "\n",
    "# remove the lines with example sentences of more than 100 words\n",
    "ppl_df = ppl_df[ppl_df['Example'].map(lambda x: len(x.split()) < 100)]\n",
    "print(len(ppl_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 correlation between corpus and perplexity\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "ppl_df['Corpus nr'] = ppl_df['Corpus'].map(lambda x: 1 if x =='C1' else 2)\n",
    "\n",
    "corpus_correlation_scores = []\n",
    "\n",
    "f1 = 'Corpus nr'\n",
    "\n",
    "for f2 in ['Words CE loss', 'Words PPL', 'Examples CE loss', 'Examples PPL']:\n",
    "    \n",
    "    these_correlations = [f2]\n",
    "    for method in ['points', 'kendall', 'spearman']:\n",
    "        if method == 'spearman':\n",
    "            results = stats.spearmanr(ppl_df[[f1, f2]].values)\n",
    "\n",
    "        if method == 'points':\n",
    "            results = stats.pointbiserialr(ppl_df[[f1, f2]].values[:,0],ppl_df[[f1, f2]].values[:,1])\n",
    "\n",
    "        else: \n",
    "            results = stats.kendalltau(ppl_df[[f1, f2]].values[:,0],ppl_df[[f1, f2]].values[:,1])\n",
    "\n",
    "        these_correlations += [results.statistic, results.pvalue]\n",
    "    corpus_correlation_scores.append(these_correlations)\n",
    "\n",
    "# create dataframe with correlations \n",
    "corpus_correlation_df = pd.DataFrame(corpus_correlation_scores, columns=['Score type', 'points', 'p', 'kendall', 'p','spearman', 'p'])\n",
    "display(corpus_correlation_df.set_index(['Score type']))\n",
    "display(ppl_df.groupby(['Category', 'Corpus']).mean('Words CE loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualize corpus and perplexities \n",
    "\n",
    "columns = ['Words CE loss', 'Examples CE loss', ]\n",
    "\n",
    "# boxplot\n",
    "sns.set(style=\"darkgrid\")\n",
    "for c in columns:\n",
    "    sns.boxenplot(data=ppl_df, x='Category', y=c, hue='Corpus', palette=\"Pastel1\")\n",
    "    \n",
    "    #place legend outside top right corner of plot\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.title(f'{c} per corpus')\n",
    "    plt.savefig(f'Thesis_DG_results/plots/First_20_Corpora_{c}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "# boxplot\n",
    "columns = ['Words PPL', 'Examples PPL']\n",
    "sns.set(style=\"darkgrid\")\n",
    "for c in columns:\n",
    "    sns.boxenplot(data=ppl_df, x='Category', y=c, hue='Corpus', palette=\"Pastel1\")\n",
    "    #place legend outside top right corner of plot\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.title(f'{c} per corpus')\n",
    "    plt.savefig(f'Thesis_DG_results/plots/First_20_Corpora_{c}.png',  bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.2 Visualize corpus and perplexities \n",
    "# distributions plots \n",
    "\n",
    "stat = 'percent'\n",
    "print(sum(ppl_df['Corpus'] == 'C1'))\n",
    "print(sum((ppl_df['Corpus'] == 'C2') & (ppl_df['Category'] != 'emerging')))\n",
    "\n",
    "\n",
    "print(\"Excluding emerging\")\n",
    "sns.histplot(data=ppl_df[ppl_df['Category'] != 'emerging'], x='Words CE loss', kde=True, hue='Corpus', stat=stat)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=ppl_df[ppl_df['Category'] != 'emerging'], x='Examples CE loss', kde=True, hue='Corpus', stat=stat)\n",
    "plt.show()\n",
    "\n",
    "print(\"Including emerging\")\n",
    "sns.histplot(data=ppl_df, x='Words CE loss', kde=True, hue='Corpus', stat=stat)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=ppl_df, x='Examples CE loss', kde=True, hue='Corpus', stat=stat)\n",
    "plt.show()\n",
    "\n",
    "print(\"Not looking at corpus\")\n",
    "sns.histplot(data=ppl_df, x='Words CE loss', kde=True,stat=stat)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=ppl_df, x='Examples CE loss', kde=True,  stat=stat)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"C1\")\n",
    "# distributions plots \n",
    "sns.histplot(data=ppl_df[ppl_df['Corpus'] == 'C1'], x='Words CE loss', kde=True, hue='Category', stat=stat)\n",
    "plt.show()\n",
    "# distributions plots \n",
    "sns.histplot(data=ppl_df[ppl_df['Corpus'] == 'C1'], x='Examples CE loss', kde=True, hue='Category', stat=stat)\n",
    "plt.show()\n",
    "\n",
    "print(\"C2\")\n",
    "sns.histplot(data=ppl_df[ppl_df['Corpus'] =='C2'], x='Words CE loss', kde=True, hue='Category', stat=stat)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=ppl_df[ppl_df['Corpus'] =='C2'], x='Examples CE loss', kde=True, hue='Category', stat=stat)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualize corpus and perplexities \n",
    "\n",
    "# visualise cross entropy loss \n",
    "for c2 in ['stable', 'changing']:\n",
    "    for c1 in ['C1', 'C2']:    \n",
    "        sns.histplot(data=ppl_df[(ppl_df['Corpus'] == c1) & (ppl_df['Category'] == c2)], \n",
    "                     x='Examples CE loss',\n",
    "                     kde=True, label=f'{c1} {c2}', stat='percent')\n",
    "        \n",
    "sns.histplot(data=ppl_df[(ppl_df['Corpus'] == 'C2') & (ppl_df['Category'] == 'emerging')],\n",
    "             x='Examples CE loss', \n",
    "             kde=True, label=f'C2 emerging', stat='percent')\n",
    "\n",
    "plt.title('Cross entropy loss of example sentences')\n",
    "plt.legend()\n",
    "plt.savefig('Cross_entropy_example_sentences_all_20.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss of target word  \n",
    "\n",
    "for c2 in ['stable', 'changing']:\n",
    "    for c1 in ['C1', 'C2']:    \n",
    "        sns.histplot(data=ppl_df[(ppl_df['Corpus'] == c1) & (ppl_df['Category'] == c2)], \n",
    "                     x='Words CE loss',\n",
    "                     kde=True, label=f'{c1} {c2}', stat='percent')\n",
    "        \n",
    "sns.histplot(data=ppl_df[(ppl_df['Corpus'] == 'C2') & (ppl_df['Category'] == 'emerging')],\n",
    "             x='Words CE loss', \n",
    "             kde=True, label=f'C2 emerging', stat='percent')\n",
    "\n",
    "plt.title('Cross entropy loss of Words CE loss')\n",
    "plt.legend()\n",
    "# plt.savefig('Cross_entropy_example_sentences_all_20.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=ppl_df, x='Words CE loss', y='Examples PPL')\n",
    "plt.show()\n",
    "\n",
    "''' There is one outlier'''\n",
    "\n",
    "print(ppl_df.iloc[ppl_df['Words CE loss'].idxmax()])\n",
    "print(ppl_df.iloc[ppl_df['Examples PPL'].idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Perplexity scores for the Annotated sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_annotations = pd.read_csv('Annotations.tsv', sep='\\t')\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute scores \n",
    "\n",
    "import time \n",
    "\n",
    "# calculate the perplexity of the sequences\n",
    "start = time.time()\n",
    "\n",
    "# initiate lists \n",
    "words_nll =  []\n",
    "examples_nll = []\n",
    "\n",
    "for target_word, example, definition in tqdm(df_annotations[['Word', 'Example', 'Prediction']].values): \n",
    "\n",
    "    ### loss of target word in example sentence \n",
    "    words_nll.append(loss_in_context(example, target_word, tokenizer, model))\n",
    "    \n",
    "    ### calculate values for example sentences\n",
    "    example_encoding = tokenizer(example, return_tensors=\"pt\")\n",
    "    examples_nll.append(nll(example_encoding, model))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"This took\", end-start)\n",
    "\n",
    "# write results \n",
    "import numpy as np\n",
    "results_df = df_annotations[['Word_id', 'Example', 'Prediction', 'Corpus', 'Category', \n",
    "                             'Boolean majority', 'Averaged judgements', 'Kaya', 'Hanna', 'Laura']]\n",
    "\n",
    "# add everything to data frame\n",
    "results_df['Words NLL'] = words_nll\n",
    "results_df['Words PPL'] = results_df['Words NLL'].map(lambda x: np.exp(x))\n",
    "\n",
    "results_df['Examples NLL'] = examples_nll\n",
    "results_df['Examples PPL'] = results_df['Examples NLL'].map(lambda x: np.exp(x))\n",
    "\n",
    "display(results_df.head(6))\n",
    "\n",
    "\n",
    "# now do pseudo perplexity\n",
    "pseudo_examples = []\n",
    "\n",
    "for target_word, example, definition in tqdm(df_annotations[['Word', 'Example', 'Prediction']].values): \n",
    "\n",
    "    ### loss of target word in example sentence \n",
    "    pseudo_examples.append(pseudo_perplexity2(example, model, tokenizer))\n",
    "\n",
    "results_df['PLL Examples'] = pseudo_examples\n",
    "results_df.to_csv('Perplexity_Annotations.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2:  Analysis of the perplexity scores Annotations \n",
    "\n",
    "- correlations\n",
    "- visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppl_df = pd.read_csv('Perplexity_Annotations.tsv' , sep='\\t')\n",
    "ppl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_perplexities = []\n",
    "\n",
    "print()\n",
    "for c1 in ['C1', 'C2']:\n",
    "    subdf = ppl_df[(ppl_df['Corpus'] == c1 )]\n",
    "    N = sum(subdf['Example'].map(lambda x: len(x.split())))\n",
    "    ppls = subdf['Examples PLL']\n",
    "    PPPL = np.exp(-sum(ppls)/N)\n",
    "\n",
    "    pseudo_perplexities.append([ c1, '-', PPPL])\n",
    "    \n",
    "print()\n",
    "for c2 in ['stable', 'changing', 'emerging']:\n",
    "    subdf = ppl_df[(ppl_df['Category'] == c2)]\n",
    "    N = sum(subdf['Example'].map(lambda x: len(x.split())))\n",
    "    ppls = subdf['Examples PLL']\n",
    "    PPPL = np.exp(-sum(ppls)/N)\n",
    "\n",
    "    pseudo_perplexities.append(['-', c2, PPPL])\n",
    "    \n",
    "# emerging \n",
    "for c1 in ['C1', 'C2']:\n",
    "    for c2 in ['stable', 'changing']:\n",
    "        subdf = ppl_df[(ppl_df['Corpus'] == c1 ) & (ppl_df['Category'] == c2)]\n",
    "        N = sum(subdf['Example'].map(lambda x: len(x.split())))\n",
    "        ppls = subdf['Examples PLL']\n",
    "        PPPL = np.exp(-sum(ppls)/N)\n",
    "        pseudo_perplexities.append([c1, c2, PPPL])\n",
    "\n",
    "# now for emerging \n",
    "c1, c2 = 'C2', 'emerging'\n",
    "subdf = ppl_df[(ppl_df['Corpus'] == c1 ) & (ppl_df['Category'] == c2)]\n",
    "N = sum(subdf['Example'].map(lambda x: len(x.split())))\n",
    "ppls = subdf['Examples PLL']\n",
    "PPPL = np.exp(-sum(ppls)/N)\n",
    "pseudo_perplexities.append([c1, c2, PPPL])\n",
    "\n",
    "pseudo_df = pd.DataFrame(pseudo_perplexities, columns=['Corpus', 'Category', 'Pseudo-perplexity'])\n",
    "pseudo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization \n",
    "\n",
    "Annotation correctness V.S. perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppl_df['Label'] = ppl_df['Boolean majority'].map(lambda x: 'incorrect' if x == 0 else 'correct')\n",
    "palette_dict = {'correct':'g', 'incorrect':'r'}\n",
    "\n",
    "# boolean majority vote\n",
    "sns.set(font_scale=2)\n",
    "columns = ['Words CE loss', 'Words PPL', 'Examples CE loss', 'Examples PPL', 'Examples PLL']\n",
    "for c in columns:\n",
    "    sns.boxplot(data=ppl_df, x='Category', y=c, hue='Label' , palette=palette_dict)\n",
    "    \n",
    "    #place legend outside top right corner of plot\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.title(f'{c}')\n",
    "    plt.savefig(f'Thesis_DG_results/plots/Annotators_Boolean_majority_{c}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average vote\n",
    "for c in columns:\n",
    "    sns.boxplot(data=ppl_df, x='Category', y=c, hue='Averaged judgements', palette=\"Spectral\")\n",
    "    \n",
    "    #place legend outside top right corner of plot\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.title(f'{c} per correctness' )\n",
    "    \n",
    "    plt.savefig(f'Thesis_DG_results/plots/Annotators_Averaged_judgements_{c}.png',  bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplots \n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.scatterplot(data=ppl_df, x='Words CE loss', y='Examples PPL', hue='Boolean majority', style='Corpus', palette=\"Spectral\")\n",
    "plt.show()\n",
    "sns.scatterplot(data=ppl_df, x='Words CE loss', y='Examples PLL', hue='Boolean majority', style='Corpus', palette=\"Spectral\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=ppl_df, x='Examples PPL', y='Examples PLL', hue='Boolean majority', style='Corpus', palette=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ppl v.s. corpus of annotations \n",
    "columns = ['Words CE loss', 'Words PPL', 'Examples CE loss', 'Examples PPL', 'Examples PLL']\n",
    "\n",
    "# boxplot\n",
    "sns.set(style=\"darkgrid\")\n",
    "for c in columns:\n",
    "    sns.boxplot(data=ppl_df, x='Category', y=c, hue='Corpus', palette=\"Pastel1\")\n",
    "    \n",
    "    #place legend outside top right corner of plot\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(f'Thesis_DG_results/plots/Annotators_Corpora_{c}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation correctness vs. perplexity of annotations \n",
    "\n",
    "# pearson correlation\n",
    "# p-values https://stackoverflow.com/questions/25571882/pandas-columns-correlation-with-statistical-significance\n",
    "\n",
    "correctness_correlation_scores = []\n",
    "for method in ['points', 'kendall', 'spearman']:\n",
    "    print(method)\n",
    "    for f1 in ['Boolean majority', 'Averaged judgements', 'Summed judgements']:\n",
    "\n",
    "        for f2 in ['Words CE loss', 'Words PPL', 'Examples CE loss', 'Examples PPL', 'Examples PLL']:\n",
    "            \n",
    "            if method == 'spearman':\n",
    "                results = stats.spearmanr(ppl_df[[f1, f2]].values)\n",
    "\n",
    "            if method == 'points':\n",
    "                results = stats.pointbiserialr(ppl_df[[f1, f2]].values[:,0],ppl_df[[f1, f2]].values[:,1])\n",
    "\n",
    "            else: \n",
    "                results = stats.kendalltau(ppl_df[[f1, f2]].values[:,0],ppl_df[[f1, f2]].values[:,1])\n",
    "                correlation = ppl_df[[f1, f2]].corr(method=method)[f1][1]\n",
    "                \n",
    "            correctness_correlation_scores.append([method, f1, f2, results.statistic, results.pvalue])\n",
    "\n",
    "correctness_correlation_df = pd.DataFrame(correctness_correlation_scores, columns=['Method', 'Judgement aggregation', 'Score type', 'correlation', 'p'])\n",
    "display(correctness_correlation_df.set_index(['Method', 'Judgement aggregation']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Some more analysis\n",
    "\n",
    "Which words+examples have the highest loss/perplexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualizations\n",
    "import seaborn.objects as so\n",
    "\n",
    "ppl_df['Word'] = ppl_df['Word_id'].map(lambda x: x.split(\"%\")[0])\n",
    "sns.set_theme()\n",
    "sns.despine()\n",
    "sns.set_context('paper')\n",
    "# sns.set(rc={'figure.figsize':(15, 15)})\n",
    "\n",
    "ax = so.Plot(data=ppl_df, x='Words CE loss', y='Examples PPL', text='Word', color='Boolean majority').add(so.Dot()).add(so.Text(color='black', halign='left',fontsize=6))\n",
    "ax.limit(x=(0 , 1.5), y=(0 ,0.8)) # this does not work\n",
    "ax.show()\n",
    "\n",
    "ax = so.Plot(data=ppl_df, x='Words CE loss', y='Examples PLL', text='Word').add(so.Dot()).add(so.Text(color='black', halign='left',fontsize=6))\n",
    "ax.limit(x=(0 , 1.5), y=(0 ,0.8))\n",
    "ax.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the highest scores are \n",
    "print(ppl_df['Words CE loss'].idxmax())\n",
    "print(ppl_df['Examples PPL'].idxmax())\n",
    "print(ppl_df['Examples PLL'].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index in [226, 393, 152]: \n",
    "    print(ppl_df.iloc[index]['Word_id'])\n",
    "    print(ppl_df.iloc[index]['Example'])\n",
    "    print(ppl_df.iloc[index]['Boolean majority'])\n",
    "    print(ppl_df.iloc[index]['Prediction'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "thesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
